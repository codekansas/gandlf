{
    "docs": [
        {
            "location": "/",
            "text": "Generative Adversarial Network Deep Learning Framework\n\n\n\n\nHome is now behind you, the world is ahead!\n\n\n\n\nThis framework was built to make it possible for anyone to train \nGenerative Adversarial Networks\n. It is built on top of \nKeras\n.\n\n\nBecause it's built on top of Keras, it has the benefits of being \nmodular\n, \nminimal\n and \nextensible\n, running on both CPU and GPU using either Tensorflow or Theano.\n\n\nInstallation\n\n\nInstalling PyPi version:\n\n\npip install gandlf\n\n\n\n\nInstalling up-to-date version:\n\n\npip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models\n\n\n\n\nInstalling from source:\n\n\ngit clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install\n\n\n\n\nQuick Start\n\n\nBelow demonstrates how a Gandlf model works.\n\n\nimport keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n# <input_data> represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -> 1\n# and fake 'generated data' -> 0, while the generator learns\n# 'generated data' -> 1.\nmodel.fit(['normal', <input_data>], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal', <input_data>], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': <input_data>},\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': <input_data>},\n          {'src_gen': '1', 'src_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([<input_data>])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\nmodel.generator.save('/generator/save/path')\nmodel.discriminator.save('/discriminator/save/path')\n\n\n\n\nGuiding Principles\n\n\nIn no particular order:\n\n\n\n\nKeras-esque\n: The APIs should feel familiar for Keras users, with some minor changes.\n\n\nPowerful\n: Models should support a wide variety of GAN architectures.\n\n\nExtensible\n: Models should be easy to modify for different experiments.\n\n\n\n\nIssues Etiquette\n\n\nMore examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the \nexamples\n directory. Just create a pull request for it.\n\n\nContribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).\n\n\nIf you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.",
            "title": "Home"
        },
        {
            "location": "/#generative-adversarial-network-deep-learning-framework",
            "text": "Home is now behind you, the world is ahead!   This framework was built to make it possible for anyone to train  Generative Adversarial Networks . It is built on top of  Keras .  Because it's built on top of Keras, it has the benefits of being  modular ,  minimal  and  extensible , running on both CPU and GPU using either Tensorflow or Theano.",
            "title": "Generative Adversarial Network Deep Learning Framework"
        },
        {
            "location": "/#installation",
            "text": "Installing PyPi version:  pip install gandlf  Installing up-to-date version:  pip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models  Installing from source:  git clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install",
            "title": "Installation"
        },
        {
            "location": "/#quick-start",
            "text": "Below demonstrates how a Gandlf model works.  import keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n# <input_data> represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -> 1\n# and fake 'generated data' -> 0, while the generator learns\n# 'generated data' -> 1.\nmodel.fit(['normal', <input_data>], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal', <input_data>], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': <input_data>},\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': <input_data>},\n          {'src_gen': '1', 'src_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([<input_data>])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\nmodel.generator.save('/generator/save/path')\nmodel.discriminator.save('/discriminator/save/path')",
            "title": "Quick Start"
        },
        {
            "location": "/#guiding-principles",
            "text": "In no particular order:   Keras-esque : The APIs should feel familiar for Keras users, with some minor changes.  Powerful : Models should support a wide variety of GAN architectures.  Extensible : Models should be easy to modify for different experiments.",
            "title": "Guiding Principles"
        },
        {
            "location": "/#issues-etiquette",
            "text": "More examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the  examples  directory. Just create a pull request for it.  Contribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).  If you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.",
            "title": "Issues Etiquette"
        },
        {
            "location": "/background/",
            "text": "What is a GAN?\n\n\nGAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.\n\n\n\n\nThe GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.\n\n\nGANs were introduced by Ian Goodfellow in an \neponymous paper\n. Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular \nKeras\n library, and therefore supports training in both Theano and Tensorflow.\n\n\nWhy are GANs interesting?\n\n\nGANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).\n\n\nTo understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.\n\n\nWhat is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.\n\n\nResources\n\n\nBelow are some resources for learning more about GANs.\n\n\n\n\n/r/MachineLearning\n\n\nAdversarial Training Facebook Group",
            "title": "Background"
        },
        {
            "location": "/background/#what-is-a-gan",
            "text": "GAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.   The GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.  GANs were introduced by Ian Goodfellow in an  eponymous paper . Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular  Keras  library, and therefore supports training in both Theano and Tensorflow.",
            "title": "What is a GAN?"
        },
        {
            "location": "/background/#why-are-gans-interesting",
            "text": "GANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).  To understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.  What is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.",
            "title": "Why are GANs interesting?"
        },
        {
            "location": "/background/#resources",
            "text": "Below are some resources for learning more about GANs.   /r/MachineLearning  Adversarial Training Facebook Group",
            "title": "Resources"
        },
        {
            "location": "/model/",
            "text": "Gandlf models employ a few tricks to make training Generative Adversarial Networks much easier than they would be in conventional Keras code.\n\n\nInputs\n\n\n\n\nThe principle of generating small amounts of finite improbability by simply hooking the logic circuits of a Bambleweeny 57 Sub-Meson Brain to an atomic vector plotter suspended in a strong Brownian Motion producer (say a nice hot cup of tea) were of course well understood...\n\n\n\n\nConceptually, the Generator part of a Generative Adversarial Network learns to map a random data distribution to the distribution of real data. When building them, you therefore need a good way to produce random latent variables. In Gandlf, this can be done easily, as follows:\n\n\nlatent_vec = keras.layers.Input(shape, name='latent_vec')\ngenerator_model = keras.models.Model(input=[latent_vec], ...)\nmodel = gandlf.Model(generator=generator_model, discriminator=...)\nmodel.compile(optimizer, loss)\nmodel.fit(x=['normal', <real_data>], ...)\n\n\n\n\nBy specifying \nnormal\n as the input to \nlatent_vec\n, the model will pull random data from a normal distribution to populate the \nlatent_vec\n input whenever it is needed. Similarly, \nuniform\n can be used for the same purpose. By default, the distribution has a mean of zero and variance of one. To change this, a function can be passed which takes as an argument \nbatch_size\n and returns the desired Numpy array:\n\n\nimport numpy as np\nfunc = lambda batch_size: np.random.normal(loc=1., scale=0.1, size=(batch_size, 3, 4))\nmodel.fit(x=[func, <real_data>], ...)\n\n\n\n\nModes\n\n\nGenerative adversarial networks have two modes: the generator part and the discriminator part. In Gandlf, the discriminator part is also separated into two modes; learning to push the generated data towards \nfake\n and to push the real data towards \nreal\n.\n\n\nA typical discriminator model might have two outputs: one to predict if the sample being recieved is real or fake data, and one to act as an auxiliary classifier (in the example of MNIST digits, the auxiliary classifier might predict which digit the sample belongs to). This can be done in Gandlf as follows:\n\n\nis_it_real = keras.layers.SomeLayer(..., name='is_it_real')\naux_class = keras.layers.SomeLayer(..., name='aux_class')\ndiscriminator = Model(inputs=[...], outputs=[is_it_real, aux_class])\nmodel = gandlf.Model(generator=..., discriminator=discriminator)\n\n\n\n\nOne problem is that the outputs have to be specified for each mode. The diagram below illustrates the three training modes:\n\n\n\n\nThe suffixes above are used in Gandlf naming to specify the outputs. The example below illustrates how to specify the desired outputs for the \nfit\n function on the model above:\n\n\nmodel.fit(inputs=[...], outputs={\n    # Tells the generator to make the discriminator's is_it_real output\n    # go towards 1 for generated samples.\n    'is_it_real_gen': 'ones',\n\n    # Tells the generator to generate samples that are classified as\n    # their correct classification by the discriminator.\n    'aux_class_gen': <correct_classes>\n\n    # Tells the discriminator to make the is_it_real output go towards 0\n    # for generated samples.\n    'is_it_real_fake': 'zeros',\n\n    # Tells the discriminator to classify the fake samples as\n    # some class (this can be turned off using loss_weights).\n    'aux_class_fake': <some_classes>\n\n    # Tells the discriminator to make the is_it_real output go towards 1\n    # for real samples.\n    'is_it_real_real': 'ones',\n\n    # Tells the discriminator to classify the real samples as their\n    # correct classes.\n    'aux_class_real': <correct_classes>\n})\n\n\n\n\nThe \nfit\n function can be written slightly more compactly by combining the \ngen\n and \nreal\n target classes. This is written as:\n\n\nmodel.fit(inputs=[...], outputs={\n    'is_it_real_gen_real': 'ones', 'is_it_real_fake': 'zeros',\n    'aux_class_real_gen': <correct_classes>, 'aux_class_fake': <some_classes>,\n})\n\n\n\n\nSimilar shorthand can be used when specifying \noptimizer\n and \nloss\n in the \ncompile\n method. For the optimizer, passing a list or tuple of optimizers with length 2 will assign the first optimizer to train the discriminator and the second optimizer to train the generator. For the loss, the same naming conventions apply:\n\n\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real': 'binary_crossentropy',\n    'aux_class': 'categorical_crossentropy',\n})\n\n# This method is equivalent, but much longer and more redundant.\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real_gen': 'binary_crossentropy',\n    'is_it_real_real': 'binary_crossentropy',\n    'is_it_real_fake': 'binary_crossentropy',\n\n    'aux_class_gen_real': 'categorical_crossentropy',\n    'aux_class_fake': 'categorical_crossentropy',\n})",
            "title": "Model"
        },
        {
            "location": "/model/#inputs",
            "text": "The principle of generating small amounts of finite improbability by simply hooking the logic circuits of a Bambleweeny 57 Sub-Meson Brain to an atomic vector plotter suspended in a strong Brownian Motion producer (say a nice hot cup of tea) were of course well understood...   Conceptually, the Generator part of a Generative Adversarial Network learns to map a random data distribution to the distribution of real data. When building them, you therefore need a good way to produce random latent variables. In Gandlf, this can be done easily, as follows:  latent_vec = keras.layers.Input(shape, name='latent_vec')\ngenerator_model = keras.models.Model(input=[latent_vec], ...)\nmodel = gandlf.Model(generator=generator_model, discriminator=...)\nmodel.compile(optimizer, loss)\nmodel.fit(x=['normal', <real_data>], ...)  By specifying  normal  as the input to  latent_vec , the model will pull random data from a normal distribution to populate the  latent_vec  input whenever it is needed. Similarly,  uniform  can be used for the same purpose. By default, the distribution has a mean of zero and variance of one. To change this, a function can be passed which takes as an argument  batch_size  and returns the desired Numpy array:  import numpy as np\nfunc = lambda batch_size: np.random.normal(loc=1., scale=0.1, size=(batch_size, 3, 4))\nmodel.fit(x=[func, <real_data>], ...)",
            "title": "Inputs"
        },
        {
            "location": "/model/#modes",
            "text": "Generative adversarial networks have two modes: the generator part and the discriminator part. In Gandlf, the discriminator part is also separated into two modes; learning to push the generated data towards  fake  and to push the real data towards  real .  A typical discriminator model might have two outputs: one to predict if the sample being recieved is real or fake data, and one to act as an auxiliary classifier (in the example of MNIST digits, the auxiliary classifier might predict which digit the sample belongs to). This can be done in Gandlf as follows:  is_it_real = keras.layers.SomeLayer(..., name='is_it_real')\naux_class = keras.layers.SomeLayer(..., name='aux_class')\ndiscriminator = Model(inputs=[...], outputs=[is_it_real, aux_class])\nmodel = gandlf.Model(generator=..., discriminator=discriminator)  One problem is that the outputs have to be specified for each mode. The diagram below illustrates the three training modes:   The suffixes above are used in Gandlf naming to specify the outputs. The example below illustrates how to specify the desired outputs for the  fit  function on the model above:  model.fit(inputs=[...], outputs={\n    # Tells the generator to make the discriminator's is_it_real output\n    # go towards 1 for generated samples.\n    'is_it_real_gen': 'ones',\n\n    # Tells the generator to generate samples that are classified as\n    # their correct classification by the discriminator.\n    'aux_class_gen': <correct_classes>\n\n    # Tells the discriminator to make the is_it_real output go towards 0\n    # for generated samples.\n    'is_it_real_fake': 'zeros',\n\n    # Tells the discriminator to classify the fake samples as\n    # some class (this can be turned off using loss_weights).\n    'aux_class_fake': <some_classes>\n\n    # Tells the discriminator to make the is_it_real output go towards 1\n    # for real samples.\n    'is_it_real_real': 'ones',\n\n    # Tells the discriminator to classify the real samples as their\n    # correct classes.\n    'aux_class_real': <correct_classes>\n})  The  fit  function can be written slightly more compactly by combining the  gen  and  real  target classes. This is written as:  model.fit(inputs=[...], outputs={\n    'is_it_real_gen_real': 'ones', 'is_it_real_fake': 'zeros',\n    'aux_class_real_gen': <correct_classes>, 'aux_class_fake': <some_classes>,\n})  Similar shorthand can be used when specifying  optimizer  and  loss  in the  compile  method. For the optimizer, passing a list or tuple of optimizers with length 2 will assign the first optimizer to train the discriminator and the second optimizer to train the generator. For the loss, the same naming conventions apply:  model.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real': 'binary_crossentropy',\n    'aux_class': 'categorical_crossentropy',\n})\n\n# This method is equivalent, but much longer and more redundant.\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real_gen': 'binary_crossentropy',\n    'is_it_real_real': 'binary_crossentropy',\n    'is_it_real_fake': 'binary_crossentropy',\n\n    'aux_class_gen_real': 'categorical_crossentropy',\n    'aux_class_fake': 'categorical_crossentropy',\n})",
            "title": "Modes"
        },
        {
            "location": "/tricks/",
            "text": "General\n\n\nA list of useful GAN hacks is available \nhere\n. Most of the tricks listed below are taken from that list, with references for how to implement them in Gandlf.\n\n\nData\n\n\n\n\nNormalize the data to \n[-1, 1]\n\n\nThis data can then be approximated using the \ntanh\n function\n\n\n\n\n\n\nUse Batch Normalization (implemented in Keras as \nBatchNormalization\n)\n\n\n\n\nModel\n\n\n\n\nUse \nkeras.layers.LeakyReLU\n instead of the ReLU activation function\n\n\nIf available, use labeled data (Auxiliary Classifier GAN or Conditional GAN). These can be integrated as inputs and outputs or as \nattention\n components.\n\n\nUse dropout in both the train and test phase: This is implemented as \ngandlf.layers.PermanentDropout\n\n\nUse minibatch discrimination: Compare the intrabatch similarities and add add it as a feature for the discriminator. This helps the model avoid generating only type of output. This is implemented as \ngandlf.layers.BatchSimilarity\n.\n\n\n\n\nTraining\n\n\n\n\nModified loss function: Instead of training the Generator to minimize \nlog(1-D)\n, train it to minimize \n-log(D)\n. This is implemented as \ngandlf.losses.negative_binary_crossentropy\n\n\nFor the latent vector(s), sample from a normal distribution instead of a uniform distribution. This is implemented as \nmodel.fit(inputs=['normal', ...)\n\n\nReinforcement learning stability tricks: Not yet implemented\n\n\nUse the Adam optimizer for the discriminator, and the SGD optimizer for the generator. This can be done as \nmodel = gandlf.Model(optimizer=['adam', 'sgd'], ...)\n\n\nDon't let the discriminator saturate!\n\n\nAdapt the generator and discriminator updates so that when the generator loss is high relative to the discriminator, its learning rate is also higher. This is implemented as \ngandlf.callbacks.AdaptiveLearningRate",
            "title": "Tricks"
        },
        {
            "location": "/tricks/#general",
            "text": "A list of useful GAN hacks is available  here . Most of the tricks listed below are taken from that list, with references for how to implement them in Gandlf.",
            "title": "General"
        },
        {
            "location": "/tricks/#data",
            "text": "Normalize the data to  [-1, 1]  This data can then be approximated using the  tanh  function    Use Batch Normalization (implemented in Keras as  BatchNormalization )",
            "title": "Data"
        },
        {
            "location": "/tricks/#model",
            "text": "Use  keras.layers.LeakyReLU  instead of the ReLU activation function  If available, use labeled data (Auxiliary Classifier GAN or Conditional GAN). These can be integrated as inputs and outputs or as  attention  components.  Use dropout in both the train and test phase: This is implemented as  gandlf.layers.PermanentDropout  Use minibatch discrimination: Compare the intrabatch similarities and add add it as a feature for the discriminator. This helps the model avoid generating only type of output. This is implemented as  gandlf.layers.BatchSimilarity .",
            "title": "Model"
        },
        {
            "location": "/tricks/#training",
            "text": "Modified loss function: Instead of training the Generator to minimize  log(1-D) , train it to minimize  -log(D) . This is implemented as  gandlf.losses.negative_binary_crossentropy  For the latent vector(s), sample from a normal distribution instead of a uniform distribution. This is implemented as  model.fit(inputs=['normal', ...)  Reinforcement learning stability tricks: Not yet implemented  Use the Adam optimizer for the discriminator, and the SGD optimizer for the generator. This can be done as  model = gandlf.Model(optimizer=['adam', 'sgd'], ...)  Don't let the discriminator saturate!  Adapt the generator and discriminator updates so that when the generator loss is high relative to the discriminator, its learning rate is also higher. This is implemented as  gandlf.callbacks.AdaptiveLearningRate",
            "title": "Training"
        },
        {
            "location": "/layers/core/",
            "text": "core.py\n\n\nCore layers, tricks which can help easily improve GANs in many cases.\n\n\nPermanentDropout\n\n\ngandlf.layers.PermanentDropout()\n\n\n\n\nAn alternative to Keras \nDropout\n which stays active during both training and testing.\n\n\nBatchSimilarity\n\n\ngandlf.layers.BatchSimilarity(similarity='exp_l1')\n\n\n\n\nCalculates the minibatch similarities, a trick introduced in \nImproved Techniques for Training GANs\n. These similarities can be added as features for the existing layer by using a Merge layer. The layer takes as input a 2D Tensor with shape \n(batch_size, num_features)\n and outputs a Tensor with shape \n(batch_size, num_similarities)\n, where \nnum_similarities\n is the total number of computed similarities.\n\n\nIn order to make this layer linear time with respect to the batch size, instead of doing a pairwise comparison between each pair of samples in the batch, for each sample a random sample is uniformly selected with which to do pairwise comparison.\n\n\nThe \nsimilarity\n argument can be one of:\n\n\n\n\nexp_l1\n: \nexp(sum(abs(a - b)))\n\n\nexp_l2\n or \nrbf\n: \nexp(sum(square(a - b)))\n\n\nl1\n: \nsum(abs(a - b))\n\n\nl2\n: \nsum(square(a - b))\n\n\ncosine\n: \ndot(a, b) / (|| a || * || b ||)\n\n\nsigmoid\n: \nsigmoid(dot(a, b))\n\n\neuclidean\n: \n1 / (1 + sum(square(a - b)))\n\n\ngeometric\n: \nsigmoid(a, b) * euclidean(a, b)\n\n\narithmetic\n: \n(sigmoid(a, b) + euclidean(a, b)) / 2\n\n\n\n\nThese implementations can be found in \nsimilarities.py\n.\n\n\nAlternatively, a function can be provided which take two tensors and returns their similarity. Multiple similarity arguments can be provided as a list or tuple. \n\n\nThe following example illustrates how to merge the similarity features with the layer output:\n\n\nsims = gandlf.layers.BatchSimilarity(['sim1', 'sim2', etc.])(input_layer)\noutput_layer = keras.layers.merge([input_layer, sims], mode='concat')",
            "title": "Core"
        },
        {
            "location": "/layers/core/#permanentdropout",
            "text": "gandlf.layers.PermanentDropout()  An alternative to Keras  Dropout  which stays active during both training and testing.",
            "title": "PermanentDropout"
        },
        {
            "location": "/layers/core/#batchsimilarity",
            "text": "gandlf.layers.BatchSimilarity(similarity='exp_l1')  Calculates the minibatch similarities, a trick introduced in  Improved Techniques for Training GANs . These similarities can be added as features for the existing layer by using a Merge layer. The layer takes as input a 2D Tensor with shape  (batch_size, num_features)  and outputs a Tensor with shape  (batch_size, num_similarities) , where  num_similarities  is the total number of computed similarities.  In order to make this layer linear time with respect to the batch size, instead of doing a pairwise comparison between each pair of samples in the batch, for each sample a random sample is uniformly selected with which to do pairwise comparison.  The  similarity  argument can be one of:   exp_l1 :  exp(sum(abs(a - b)))  exp_l2  or  rbf :  exp(sum(square(a - b)))  l1 :  sum(abs(a - b))  l2 :  sum(square(a - b))  cosine :  dot(a, b) / (|| a || * || b ||)  sigmoid :  sigmoid(dot(a, b))  euclidean :  1 / (1 + sum(square(a - b)))  geometric :  sigmoid(a, b) * euclidean(a, b)  arithmetic :  (sigmoid(a, b) + euclidean(a, b)) / 2   These implementations can be found in  similarities.py .  Alternatively, a function can be provided which take two tensors and returns their similarity. Multiple similarity arguments can be provided as a list or tuple.   The following example illustrates how to merge the similarity features with the layer output:  sims = gandlf.layers.BatchSimilarity(['sim1', 'sim2', etc.])(input_layer)\noutput_layer = keras.layers.merge([input_layer, sims], mode='concat')",
            "title": "BatchSimilarity"
        },
        {
            "location": "/layers/attention/",
            "text": "attention.py\n\n\nRecurrentAttention1D\n\n\ngandlf.layers.RecurrentAttention1D(layer, attention, attn_activation='tanh', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer)\n\n\n\n\nMakes the wrapped Keras RNN \nlayer\n pay attention to the \nattention\n tensor, which has shape \n(batch_size, num_attn_features)\n.\n\n\nThe updated hidden state is computed after each timestep as:\n\n\ntrainable_params = [U_m, b_m, U_a, U_s, b_s]\n\n# Given hidden output \"h\" at each timestep:\nm = attn_activation(dot(h, U_m) + dot(attention, U_a) + b_m)\ns = attn_gate(dot(m, U_s) + b_s)\nh_new = s * h  # Element-wise weighting.\n\n\n\n\nRecurrentAttention2D\n\n\ngandlf.layers.RecurrentAttention2D(layer, attention, time_dist_activation='softmax', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer=None)\n\n\n\n\nMakes the wrapped Keras RNN \nlayer\n pay attention to the \nattention\n tensor, which has shape \n(batch_size, num_attn_timesteps, num_attn_features)\n.\n\n\nThe updated hidden state is computed after each timestep as:\n\n\ntrainable_params = [U_t, b_t, U_a, b_a]\n\n# Given hidden output \"h\" at each timestep:\nt = time_dist_activation(dot(h, U_t), b_t)\nw = sum(t * attention)  # Weights each timestep by `t`.\ns = attn_gate_func(dot(w, U_a) + b_a)\nh_new = s * h  # Element-wise weighting.",
            "title": "Attention"
        },
        {
            "location": "/layers/attention/#recurrentattention1d",
            "text": "gandlf.layers.RecurrentAttention1D(layer, attention, attn_activation='tanh', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer)  Makes the wrapped Keras RNN  layer  pay attention to the  attention  tensor, which has shape  (batch_size, num_attn_features) .  The updated hidden state is computed after each timestep as:  trainable_params = [U_m, b_m, U_a, U_s, b_s]\n\n# Given hidden output \"h\" at each timestep:\nm = attn_activation(dot(h, U_m) + dot(attention, U_a) + b_m)\ns = attn_gate(dot(m, U_s) + b_s)\nh_new = s * h  # Element-wise weighting.",
            "title": "RecurrentAttention1D"
        },
        {
            "location": "/layers/attention/#recurrentattention2d",
            "text": "gandlf.layers.RecurrentAttention2D(layer, attention, time_dist_activation='softmax', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer=None)  Makes the wrapped Keras RNN  layer  pay attention to the  attention  tensor, which has shape  (batch_size, num_attn_timesteps, num_attn_features) .  The updated hidden state is computed after each timestep as:  trainable_params = [U_t, b_t, U_a, b_a]\n\n# Given hidden output \"h\" at each timestep:\nt = time_dist_activation(dot(h, U_t), b_t)\nw = sum(t * attention)  # Weights each timestep by `t`.\ns = attn_gate_func(dot(w, U_a) + b_a)\nh_new = s * h  # Element-wise weighting.",
            "title": "RecurrentAttention2D"
        },
        {
            "location": "/layers/wrappers/",
            "text": "wrappers.py\n\n\nResidual\n\n\ngandlf.layers.Residual(layer, merge_mode='sum')\n\n\n\n\nApplies a residual to any Keras layer or model, so long as it's inputs are the same dimension as its outputs. Useful for implementing residual architectures.\n\n\nThe provided \nlayer\n has to have the same input and output dimensions. Given an input \nx\n, the output is:\n\n\noutput = merge_mode(x, layer(x))\n\n\n\n\nmerge_mode\n can be a string like for \nMerge\n or a \nMerge\n layer itself.",
            "title": "Wrappers"
        },
        {
            "location": "/layers/wrappers/#residual",
            "text": "gandlf.layers.Residual(layer, merge_mode='sum')  Applies a residual to any Keras layer or model, so long as it's inputs are the same dimension as its outputs. Useful for implementing residual architectures.  The provided  layer  has to have the same input and output dimensions. Given an input  x , the output is:  output = merge_mode(x, layer(x))  merge_mode  can be a string like for  Merge  or a  Merge  layer itself.",
            "title": "Residual"
        },
        {
            "location": "/callbacks/",
            "text": "callbacks.py\n\n\nAdaptiveLearningRate\n\n\ngandlf.callbacks.AdaptiveLearningRate(discriminator_lr, generator_lr)\n\n\n\n\nAdapts the learning rate on each batch according to the model's loss, where \ndiscriminator_lr\n and \ngenerator_lr\n are the maximum learning rates for the discriminator and generator models.\n\n\nOn each batch, the learning rate for the generator and discriminator are adapted according to the losses of each:\n\n\nbatch_dis_lr = discriminator_lr * (discriminator_loss / total_loss)\nbatch_gen_lr = generator_lr * (generator_loss / total_loss)",
            "title": "Callbacks"
        },
        {
            "location": "/callbacks/#adaptivelearningrate",
            "text": "gandlf.callbacks.AdaptiveLearningRate(discriminator_lr, generator_lr)  Adapts the learning rate on each batch according to the model's loss, where  discriminator_lr  and  generator_lr  are the maximum learning rates for the discriminator and generator models.  On each batch, the learning rate for the generator and discriminator are adapted according to the losses of each:  batch_dis_lr = discriminator_lr * (discriminator_loss / total_loss)\nbatch_gen_lr = generator_lr * (generator_loss / total_loss)",
            "title": "AdaptiveLearningRate"
        },
        {
            "location": "/losses/",
            "text": "losses.py\n\n\nNegative Binary Crossentropy\n\n\nInstead of minimizing \nlog(1-D)\n maximize \nlog(D)\n.\n\n\nNote that when using this loss function, you should not change the target. For example, if you want \nG -> 0\n and \nD -> 1\n to train your generator, you should replace your \nbinary_crossentropy\n for the fake output with \nnegative_binary_crossentropy\n while keeping the target output as 0.\n\n\nMaximize\n\n\nMaximizes \ny_true\n, regardless of \ny_pred\n.\n\n\nMinimize\n\n\nMinimizes \ny_true\n, regardless of \ny_pred\n.",
            "title": "Losses"
        },
        {
            "location": "/losses/#negative-binary-crossentropy",
            "text": "Instead of minimizing  log(1-D)  maximize  log(D) .  Note that when using this loss function, you should not change the target. For example, if you want  G -> 0  and  D -> 1  to train your generator, you should replace your  binary_crossentropy  for the fake output with  negative_binary_crossentropy  while keeping the target output as 0.",
            "title": "Negative Binary Crossentropy"
        },
        {
            "location": "/losses/#maximize",
            "text": "Maximizes  y_true , regardless of  y_pred .",
            "title": "Maximize"
        },
        {
            "location": "/losses/#minimize",
            "text": "Minimizes  y_true , regardless of  y_pred .",
            "title": "Minimize"
        },
        {
            "location": "/examples/xor/",
            "text": "xor.py\n\n\nThis is a basic example which illustrates the typical \"Hello, world!\" for neural networks. The input data consists of four uniform distributions, centered near \n(-1, -1)\n, \n(1, -1)\n, \n(-1, 1)\n and \n(1, 1)\n (illustrated below).\n\n\n\n\nThe model can either be trained in unsupervised mode or supervised mode. In the supervised mode, it acts as an auxiliary classifier GAN, which explicitly says which distribution the generated data should come from.\n\n\nWhen trained in the unsupervised mode, the data tends to cluster in one of the distributions. A potential way to fix this would be to let the GAN look at a whole batch of data, which would let it know that it's clustering too much on one distribution.",
            "title": "Hello, world!"
        },
        {
            "location": "/examples/mnist/mnist_gan/",
            "text": "mnist_gan.py\n\n\nThis example illustrates how to use a Gandlf model to generate MNIST digits. The model can be run in supervised mode (where the discriminator and generator know the desired class labels) or unsupervised mode (which is a more pure GAN implementation).\n\n\nThis example is a Gandlf implementation of the Keras MNIST ACGAN example, which can be found \nhere\n. One important distinction is that Gandlf runs the generator and discriminator updates in parallel rather than sequentially; this can't be done in Keras normally.\n\n\nThe GIF below illustrates samples from different latent vectors for each number. As the latent vector is smoothly interpolated, the image smoothly changes.\n\n\n\n\nThe next GIF illustrates smoothly interpolating between different digits.\n\n\n\n\nIn addition to the convolutional model, a simple feed-forward model can be used for the discriminator and generator which can be trained much more quickly (feasible to run it on a laptop). The samples below show randomly sampled generated images from the lite model, which took about 100 seconds per epoch on a Macbook Pro.",
            "title": "MNIST GAN"
        },
        {
            "location": "/examples/upsample/",
            "text": "upsample_gan.py\n\n\nThis is an implementation of the \nneural-enhance\n project. The model can be trained on MNIST or CIFAR data. The MNIST digits below illustrate the model. The first two digits the \"real\" data, where the first is a downsampled version of the second. The remaining digits are the results of iteratively applying the model, upsampling the original digit.\n\n\n\n\nThe same process can be done on CIFAR images:\n\n\n\n\n\n\nModel architecture\n\n\nThe generator and discriminator are both convolutional neural networks. I played around with different sized models. The MNIST and CIFAR models were trained for 10 epochs, which took about 10 minutes on a Titan X GPU. The generator architecture first had an upsampling layer, then several convolutional layers.\n\n\nThe upsampling part took advantage of the fact that the filters in the convolutional layer are dimension-invariant, so the same filters can be applied at double the resolution. To do sampling, the model is reconstructed at twice the resolution, then the trained generator weights are loaded in order to produce a new image.\n\n\nRelated links\n\n\n\n\nneural-enhance\n: A Lasagne implementation of this upsampling idea\n\n\nGenerating Large Images from Latent Vectors\n: Train a model to take (X, Y) coordinates and output a pixel intensity, then interpolate between points to get high-resolution images\n\n\nPixel Recursive Super Resolution\n: Train a PixelCNN model to un-pixelate faces",
            "title": "Upsampling GAN"
        },
        {
            "location": "/examples/upsample/#model-architecture",
            "text": "The generator and discriminator are both convolutional neural networks. I played around with different sized models. The MNIST and CIFAR models were trained for 10 epochs, which took about 10 minutes on a Titan X GPU. The generator architecture first had an upsampling layer, then several convolutional layers.  The upsampling part took advantage of the fact that the filters in the convolutional layer are dimension-invariant, so the same filters can be applied at double the resolution. To do sampling, the model is reconstructed at twice the resolution, then the trained generator weights are loaded in order to produce a new image.",
            "title": "Model architecture"
        },
        {
            "location": "/examples/upsample/#related-links",
            "text": "neural-enhance : A Lasagne implementation of this upsampling idea  Generating Large Images from Latent Vectors : Train a model to take (X, Y) coordinates and output a pixel intensity, then interpolate between points to get high-resolution images  Pixel Recursive Super Resolution : Train a PixelCNN model to un-pixelate faces",
            "title": "Related links"
        },
        {
            "location": "/examples/reverse/",
            "text": "upsample_gan.py\n\n\nThis example shows how to use gradient descent to map an image back to the latent vector that it corresponds to. This approach can be done on an untrained model or a trained model. The GIF on the left shows the gradient descent process, as the latent vector moves in the direction that causes the generator to produce the target image on the right.\n\n\n\n\n\n\nThis could be an interesting way to benchmark the GAN performance; given a real image, map it backwards to its latent vector, and see how far the latent vector is from a normal vector.",
            "title": "Reversing Generator"
        }
    ]
}